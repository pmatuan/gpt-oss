srun: job 82015 queued and waiting for resources
srun: job 82015 has been allocated resources
vocab_size: 201088
hidden_dim: 2880
n_experts: 32
experts_per_token: 4
intermediate_dim: 2880
n_layers: 24
head_dim: 64
n_attn_heads: 64
n_kv_heads: 8
max_seq_len: 2048
init context len: 4096
rope theta: 150000.000000
rope_scaling_factor: 32.000000
sliding window: 128
swiglu_limit: 7.000000
requests size = 235110400 B
Num requests: 896
Found 1 HIP devices, initializing multi-GPU setup...
Not using expert parallelism (n_experts=32, n_devices=1)
Initializing device 0 for batch size 896...
[DEVICE] 0 [HIP] before allocations: HBM free 63.90 GiB / total 63.98 GiB (used 0.09 GiB)
[DEVICE] 0 [HIP] after activations: HBM free 63.00 GiB / total 63.98 GiB (used 0.98 GiB)
[DEVICE] 0 [HIP] after small BF16 weights: HBM free 62.99 GiB / total 63.98 GiB (used 0.99 GiB)
[DEVICE] 0 [HIP] after expert biases: HBM free 62.97 GiB / total 63.98 GiB (used 1.01 GiB)
[DEVICE] 0 [HIP] after large BF16 weights: HBM free 24.03 GiB / total 63.98 GiB (used 39.95 GiB)
[DEVICE] 0 allocated KV cache full 1024 tokens
[DEVICE] 0 [HIP] after KV cache allocation (model fully loaded): HBM free 0.41 GiB / total 63.98 GiB (used 63.58 GiB)
Multi-GPU initialization complete!

warm up elapsed time(s): 98.133000
Processing 896 requests across 1 GPUs...
Multi-GPU inference completed. Total tokens generated: 891656

elapsed time(s): 238.067000, achieved throughput TPS (tok/s): 3745.399404

finish elapsed time(s): 0.001000

┌──────────────────────────────────────────────────────────────────────────────────────┐
│                                  PROFILER SUMMARY                                    │
├──────────────────────────────────────────────────────────────────────────────────────┤
│ Function/Kernel Name                          │   Total (ms) │    Calls │   Avg (ms) │
├──────────────────────────────────────────────────────────────────────────────────────┤
│ CPU:inference                                 │   238066.236 │        1 │ 238066.236 │
│ CPU:gpu_forward_device_batch                  │   238000.675 │     1023 │    232.650 │
│ GPU:mlp1_fused_gemm                           │    82643.182 │    24552 │      3.366 │
│ GPU:flash_decoding_odd                        │    49091.227 │    12276 │      3.999 │
│ GPU:mlp2_noatomic_gemm                        │    41198.995 │    24552 │      1.678 │
│ GPU:matmul_logits_kernel                      │    15897.631 │     1023 │     15.540 │
│ GPU:flash_decoding_even                       │    14027.303 │    12276 │      1.143 │
│ GPU:matmul_bias_att_kernel                    │     9722.014 │    24552 │      0.396 │
│ GPU:matmul_bias_qkv_kernel                    │     8799.503 │    24552 │      0.358 │
│ GPU:matmul_router_kernel                      │     2829.597 │    24552 │      0.115 │
│ GPU:residual_add_batch_kernel                 │     1625.870 │    49104 │      0.033 │
│ GPU:rmsnorm_batch_kernel                      │     1578.016 │    50127 │      0.031 │
│ GPU:reduce_mlp2_slots                         │     1495.331 │    24552 │      0.061 │
│ GPU:fused_split_rope_scatter_qkv_batch_kernel │      918.054 │    24552 │      0.037 │
│ GPU:fused_topk_softmax_batch_kernel           │      898.543 │    24552 │      0.037 │
│ GPU:argmax_batch_kernel                       │      665.738 │     1023 │      0.651 │
│ GPU:zero_mlp2_partial                         │      656.225 │    24552 │      0.027 │
│ GPU:expert_assignment_building                │      441.262 │    24552 │      0.018 │
│ GPU:expert_counting                           │      415.640 │    24552 │      0.017 │
│ GPU:exclusive_scan_small_kernel               │      258.717 │    24552 │      0.011 │
│ GPU:copy_embedding_bf16_batch_kernel          │       39.270 │     1023 │      0.038 │
└──────────────────────────────────────────────────────────────────────────────────────┘

