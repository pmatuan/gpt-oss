vocab_size: 201088
hidden_dim: 2880
n_experts: 32
experts_per_token: 4
intermediate_dim: 2880
n_layers: 24
head_dim: 64
n_attn_heads: 64
n_kv_heads: 8
max_seq_len: 2048
init context len: 4096
rope theta: 150000.000000
rope_scaling_factor: 32.000000
sliding window: 128
swiglu_limit: 7.000000
requests size = 16793600 B
Num requests: 64
Found 1 HIP devices, initializing multi-GPU setup...
Initializing device 0...
[DEVICE] 0 [HIP] before allocations: HBM free 63.90 GiB / total 63.98 GiB (used 0.09 GiB)
[DEVICE] 0 [HIP] after activations: HBM free 63.66 GiB / total 63.98 GiB (used 0.33 GiB)
[DEVICE] 0 [HIP] after small FP32 weights: HBM free 63.65 GiB / total 63.98 GiB (used 0.34 GiB)
[DEVICE] 0 [HIP] after expert biases: HBM free 63.62 GiB / total 63.98 GiB (used 0.37 GiB)
[DEVICE] 0 [HIP] after large BF16 weights (model loaded): HBM free 24.67 GiB / total 63.98 GiB (used 39.31 GiB)
Multi-GPU initialization complete!

warm up elapsed time(s): 55.761000
Processing 64 requests across 1 GPUs...
num experts: 32, experts per token: 4
Multi-GPU inference completed. Total tokens generated: 62275

elapsed time(s): 308.268000, achieved throughput TPS (tok/s): 202.015778

finish elapsed time(s): 0.006000
