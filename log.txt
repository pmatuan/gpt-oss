vocab_size: 201088
hidden_dim: 2880
n_experts: 32
experts_per_token: 4
intermediate_dim: 2880
n_layers: 24
head_dim: 64
n_attn_heads: 64
n_kv_heads: 8
max_seq_len: 2048
init context len: 4096
rope theta: 150000.000000
rope_scaling_factor: 32.000000
sliding window: 128
swiglu_limit: 7.000000
requests size = 134348800 B
Num requests: 512
Found 1 HIP devices, initializing multi-GPU setup...
Initializing device 0...
[DEVICE] 0 [HIP] before allocations: HBM free 63.90 GiB / total 63.98 GiB (used 0.09 GiB)
[DEVICE] 0 [HIP] after activations: HBM free 63.89 GiB / total 63.98 GiB (used 0.09 GiB)
[DEVICE] 0 [HIP] after small FP32 weights: HBM free 63.74 GiB / total 63.98 GiB (used 0.24 GiB)
[DEVICE] 0 [HIP] after expert biases: HBM free 63.71 GiB / total 63.98 GiB (used 0.27 GiB)
[DEVICE] 0 [HIP] after large BF16 weights (model loaded): HBM free 24.76 GiB / total 63.98 GiB (used 39.22 GiB)
Multi-GPU initialization complete!

warm up elapsed time(s): 45.160000
Processing 512 requests across 1 GPUs...
Multi-GPU inference completed. Total tokens generated: 508052

elapsed time(s): 514.639000, achieved throughput TPS (tok/s): 987.200737

finish elapsed time(s): 0.062000

┌──────────────────────────────────────────────────────────────────────────────────────┐
│                                  PROFILER SUMMARY                                    │
├──────────────────────────────────────────────────────────────────────────────────────┤
│ Function/Kernel Name                          │   Total (ms) │    Calls │   Avg (ms) │
├──────────────────────────────────────────────────────────────────────────────────────┤
│ CPU:inference                                 │   514638.607 │        1 │ 514638.607 │
│ CPU:gpu_forward_device_batch                  │   514043.373 │     1023 │    502.486 │
│ GPU:mlp1_fused_gemm                           │   187169.379 │    24552 │      7.623 │
│ GPU:mlp2_bias_weighted_accum_gemm             │   120751.232 │    24552 │      4.918 │
│ GPU:attention_batch_kernel                    │    73743.536 │    24552 │      3.004 │
│ GPU:matmul_bias_gemm_kernel_bf16_mfma_qkv     │    39153.316 │    24552 │      1.595 │
│ GPU:matmul_bias_gemm_kernel_bf16_mfma_att     │    37364.728 │    24552 │      1.522 │
│ GPU:matmul_gemm_kernel_bf16_mfma              │    37067.635 │     1023 │     36.234 │
│ GPU:fused_split_rope_scatter_qkv_batch_kernel │     2813.403 │    24552 │      0.115 │
│ GPU:matmul_bias_gemm_kernel_float             │     1758.901 │    24552 │      0.072 │
│ GPU:rmsnorm_batch_kernel                      │     1312.901 │    50127 │      0.026 │
│ GPU:residual_add_batch_kernel                 │     1220.618 │    49104 │      0.025 │
│ GPU:expert_counting                           │      998.732 │    24552 │      0.041 │
│ GPU:expert_assignment_building                │      861.142 │    24552 │      0.035 │
│ GPU:fused_topk_softmax_batch_kernel           │      677.102 │    24552 │      0.028 │
│ GPU:copy_embedding_bf16_batch_kernel          │       26.020 │     1023 │      0.025 │
└──────────────────────────────────────────────────────────────────────────────────────┘

